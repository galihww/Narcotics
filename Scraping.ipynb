{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Standard Library Imports:**\n",
    "    - `argparse`: Used to write user-friendly command-line interfaces. The module defines a few functions and classes to help you create a parser for command-line options, arguments, and subcommands.\n",
    "    - `concurrent.futures.ThreadPoolExecutor, wait`: Provides a high-level interface for computing asynchronously. The `ThreadPoolExecutor` allows you to launch parallel tasks in a thread pool.\n",
    "    - `datetime.date`: Provides date objects for working with date values and implementing date manipulations.\n",
    "    - `io`: A core utility for working with streams and input/output operations.\n",
    "    - `os`: Provides a way of using operating system-dependent functionality like reading or writing to the filesystem.\n",
    "    - `re`: Provides support for regular expressions, allowing for string searching and manipulation.\n",
    "    - `time`: Provides time-related functions, such as getting the current time or causing a delay in execution with `sleep()`.\n",
    "    - `urllib`: A module for fetching data across the web, providing a high-level interface for dealing with URL handling."
   ],
   "id": "ece0f244810e9d3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. **Third-party Imports:**\n",
    "    - `fitz` (PyMuPDF): A library used for handling and manipulating PDF files, providing functionalities like extracting text, images, or other document manipulations.\n",
    "    - `pandas as pd`: A powerful data manipulation and analysis library. It provides data structures and functions needed to work with structured data seamlessly.\n",
    "    - `requests`: A simple, yet elegant, HTTP library for making HTTP requests.\n",
    "    - `BeautifulSoup` from `bs4`: A library for parsing HTML and XML documents to extract data.\n",
    "    - `pdfminer.high_level`: Part of the PDFMiner library, used for text extraction from PDF documents."
   ],
   "id": "a1516b578fb56dda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:37.521657Z",
     "start_time": "2024-12-10T03:12:30.230299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Standard library imports\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from datetime import date\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "# Third-party imports\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer import high_level"
   ],
   "id": "eab7397e90b0ba6f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines a few utility functions to ensure a directory exists: `ensure_directory_exists()` checks and creates a directory if it does not exist, `create_path()` constructs a full path using the current directory and a given folder name, and `create_directory_if_not_exists()` creates the directory when it doesn't already exist.",
   "id": "b1d4bbac1693330c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:37.537133Z",
     "start_time": "2024-12-10T03:12:37.531758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "\n",
    "def ensure_directory_exists(folder_name):\n",
    "\n",
    "    path = create_path(folder_name)\n",
    "    create_directory_if_not_exists(path)\n",
    "    return path\n",
    "\n",
    "def create_path(folder_name):\n",
    "    return os.path.join(CURRENT_DIRECTORY, folder_name)\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ],
   "id": "dfeb8e63d03affb2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines a function `fetch_html_with_retries(url)` that attempts to fetch HTML content from a given URL, retrying up to 3 times with a 15-second delay between retries, using the `requests` library and parsing the content with `BeautifulSoup` using the \"lxml\" parser.",
   "id": "35e33ea2a57eba9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:37.561339Z",
     "start_time": "2024-12-10T03:12:37.542515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 15  # delay in seconds\n",
    "\n",
    "\n",
    "def fetch_html_with_retries(url):\n",
    "    attempt_count = 0\n",
    "    while attempt_count < MAX_RETRIES:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return BeautifulSoup(response.text, \"lxml\")\n",
    "        except requests.RequestException:\n",
    "            attempt_count += 1\n",
    "            time.sleep(RETRY_DELAY)"
   ],
   "id": "b4b88ba57ebd0fd2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines a function `extract_detail_from_soup(soup, keyword)` that extracts text from the next sibling tag of a `<td>` tag containing a specific keyword within a parsed HTML document (`soup`). It safely handles cases where the desired tag might not be found.",
   "id": "25c49f35b8dce64e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:38.582181Z",
     "start_time": "2024-12-10T03:12:38.566245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_detail_from_soup(soup, keyword):\n",
    "    try:\n",
    "        keyword_tag = soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
    "        if keyword_tag:\n",
    "            next_tag = keyword_tag.find_next()\n",
    "            detail_text = next_tag.get_text().strip()\n",
    "            return detail_text\n",
    "        else:\n",
    "            return \"\"\n",
    "    except AttributeError:  # This catches potential errors like 'NoneType' object has no attribute 'find_next'\n",
    "        return \"\""
   ],
   "id": "f85a21cc80293aa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines a function `download_pdf(file_url, output_directory)` that downloads a PDF file from a given URL, saves it to the specified output directory, and returns the content as a `BytesIO` object along with the filename. It handles errors using exceptions for `URLError` and `IOError`.",
   "id": "913f0693ec029b20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:38.613711Z",
     "start_time": "2024-12-10T03:12:38.598045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def download_pdf(file_url, output_directory):\n",
    "    try:\n",
    "        # Open the URL\n",
    "        response = urllib.request.urlopen(file_url)\n",
    "\n",
    "        # Extract the filename from headers\n",
    "        filename = response.info().get_filename().replace(\"/\", \" \")\n",
    "\n",
    "        # Read file content\n",
    "        pdf_content = response.read()\n",
    "\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(output_directory, filename)\n",
    "\n",
    "        # Write file content to a file\n",
    "        with open(file_path, \"wb\") as pdf_file:\n",
    "            pdf_file.write(pdf_content)\n",
    "\n",
    "        # Return the content in BytesIO and the filename\n",
    "        return io.BytesIO(pdf_content), filename\n",
    "\n",
    "    except (urllib.error.URLError, IOError) as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None, None"
   ],
   "id": "9414d70e2f14a884",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This function, `clean_text(text)`, removes specific unwanted texts and formats from the input text. It removes newline characters, unnecessary spaces, page numbers, and replaces or deletes specific unwanted phrases from the text.",
   "id": "421b53406570c0dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:38.645563Z",
     "start_time": "2024-12-10T03:12:38.629751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "def clean_text(text):\n",
    "\n",
    "    UNWANTED_TEXTS = [\n",
    "    \"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\",\n",
    "    \"Disclaimer\\n\",\n",
    "    \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\n",
    "    \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\",\n",
    "    \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\",\n",
    "    \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\",\n",
    "    \"Direktori Putusan Mahkamah Agung Republik Indonesia\",\n",
    "    \"putusan.mahkamahagung.go.id\",\n",
    "    \"hkama ahkamah Agung Repub ahkamah Agung Republik Indonesia mah Agung Republik Indonesia blik Indonesi\",\n",
    "    \"Disclaimer Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu. Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui : Email : kepaniteraan@mahkamahagung.go.id Telp : 021-384 3348 (ext.318)\",\n",
    "    ]\n",
    "\n",
    "    # remove extra space\n",
    "    text = ' '.join(text.replace('\\n', ' ').split())\n",
    "\n",
    "    # Remove pages number\n",
    "    for i in range(1, 100):\n",
    "        UNWANTED_TEXTS.append(f\"Halaman  | {i} \")\n",
    "    # Remove 'Halaman {number} dari {number}' patterns\n",
    "    text = re.sub(r'Halaman \\d+ dari \\d+', '', text)\n",
    "\n",
    "    # Replace word 'Halaman' with a newline and remove any numbers following it\n",
    "    text = re.sub(r'Halaman \\d+', '\\n', text)\n",
    "\n",
    "    for unwanted_text in UNWANTED_TEXTS:\n",
    "        text = text.replace(unwanted_text, \"\")\n",
    "    return text"
   ],
   "id": "f84156f89490f6c8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines a function `extract_and_clean_text(pdf_path)` that extracts text from a PDF file using the PyMuPDF library (`fitz`) and returns the cleaned text by removing newlines and extra spaces.",
   "id": "4135c5e944994d23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:38.677124Z",
     "start_time": "2024-12-10T03:12:38.661437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_and_clean_text(pdf_path):\n",
    "    \"\"\"Extracts and cleans text from a PDF file.\"\"\"\n",
    "    import fitz  # Importing at function level for modularity\n",
    "\n",
    "    text_content = \"\"\n",
    "    with fitz.open(pdf_path) as document:\n",
    "        for page in document:\n",
    "            text_content += page.get_text()\n",
    "\n",
    "    return clean_text(text_content)"
   ],
   "id": "cae33e4c06709ec",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code defines an `extract_data` function to scrape and extract data from a webpage, including PDF details. It then saves this data into a CSV and a text file. Key functions within this involve `process_pdf`, which downloads and processes a PDF, and `save_results`, which saves the extracted data to specified files.",
   "id": "acfb3ec37eb1a944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:38.724443Z",
     "start_time": "2024-12-10T03:12:38.708761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "PDF_AVAILABLE_TEXT = \"Ada PDF\"\n",
    "PDF_NOT_AVAILABLE_TEXT = \"Tidak ada PDF\"\n",
    "LOGGING_FILE_NAME = \"Logging.csv\"\n",
    "\n",
    "def extract_data(url, keyword_url):\n",
    "    # Extract the required details and save to both PDF and CSV simultaneously\n",
    "    path_output = create_path(\"putusan\")\n",
    "    path_pdf = create_path(\"pdf-putusan\")\n",
    "    current_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "    soup = fetch_html_with_retries(url)\n",
    "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "    judul = table.find(\"h2\").text.strip()\n",
    "    tahun = extract_detail_from_soup(table, \"Tahun\")\n",
    "    tanggal_register = extract_detail_from_soup(table, \"Tanggal Register\")\n",
    "    kaidah = extract_detail_from_soup(table, \"Kaidah\")\n",
    "\n",
    "    link_pdf, text_pdf, file_name_pdf, has_pdf = process_pdf(soup, path_pdf)\n",
    "\n",
    "    data = {\n",
    "        \"judul\": judul,\n",
    "        \"tanggal_register\": tanggal_register,\n",
    "        \"tahun\": tahun,\n",
    "        \"kaidah\": kaidah,\n",
    "        \"link\": url,\n",
    "        \"link_pdf\": link_pdf,\n",
    "        \"pdf_name\": file_name_pdf,\n",
    "        \"has_pdf\": has_pdf,\n",
    "    }\n",
    "\n",
    "    save_results(path_output, keyword_url, current_date, data, file_name_pdf, text_pdf)\n",
    "\n",
    "def process_pdf(soup, path_pdf):\n",
    "    try:\n",
    "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
    "        file_pdf, file_name_pdf = download_pdf(link_pdf, path_pdf)\n",
    "        pdf_file_path = os.path.join(path_pdf, file_name_pdf)\n",
    "        text_pdf = clean_text(extract_and_clean_text(pdf_file_path))\n",
    "        has_pdf = PDF_AVAILABLE_TEXT\n",
    "    except Exception as e:\n",
    "        link_pdf = \"\"\n",
    "        text_pdf = \"\"\n",
    "        file_name_pdf = \"\"\n",
    "        has_pdf = PDF_NOT_AVAILABLE_TEXT\n",
    "    return link_pdf, text_pdf, file_name_pdf, has_pdf\n",
    "\n",
    "def save_results(path_output, keyword_url, current_date, data, file_name_pdf, text_pdf):\n",
    "    result = pd.DataFrame([data])\n",
    "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
    "\n",
    "    destination_csv = os.path.join(path_output, LOGGING_FILE_NAME)\n",
    "    if not os.path.isfile(destination_csv):\n",
    "        result.to_csv(destination_csv, header=True, index=False)\n",
    "    else:\n",
    "        result.to_csv(destination_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    destination_txt = os.path.join(path_output, f\"{file_name_pdf}_{current_date}.txt\")\n",
    "    with open(destination_txt, 'w', encoding='utf-8') as file:\n",
    "        file.write(text_pdf)"
   ],
   "id": "26fa9446e86f8f2f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code defines three functions:\n",
    "\n",
    "1. `build_search_link(base_url, page_number, sort)`: Constructs a search URL based on input parameters, including sorting details.\n",
    "\n",
    "2. `run_process(base_url, page_number, sort_results)`: Utilizes the `build_search_link` function to get a search URL, fetches HTML content, finds decision links, and calls `extract_data` on them.\n",
    "\n",
    "3. `run_scraper(keyword=None, url=None, sort_by_date=True, download_pdfs=True)`: Acts as the main scraping function, taking in a keyword or URL and sorting preferences, scraping pages asynchronously using `ThreadPoolExecutor`.\n",
    "\n",
    "The key feature: The cell orchestrates a web scraping process to retrieve decision data based on different parameters, leveraging multi-threading for efficiency."
   ],
   "id": "b56410988f1270ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:12:38.740475Z",
     "start_time": "2024-12-10T03:12:38.724443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_search_link(base_url, page_number, sort):\n",
    "    \"\"\"Build the search link based on the given parameters.\"\"\"\n",
    "    search_link = f\"{base_url}&page={page_number}\" if base_url.startswith(\n",
    "        \"https\") else f\"https://putusan3.mahkamahagung.go.id/search.html?q={base_url}&page={page_number}\"\n",
    "    if sort:\n",
    "        search_link += \"&obf=TANGGAL_PUTUS&obm=desc\"\n",
    "    return search_link\n",
    "\n",
    "\n",
    "def run_process(base_url, page_number, sort_results):\n",
    "    search_link = build_search_link(base_url, page_number, sort_results)\n",
    "    soup = fetch_html_with_retries(search_link)\n",
    "    decision_links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
    "    for decision_link in decision_links:\n",
    "        extract_data(decision_link[\"href\"], base_url)\n",
    "\n",
    "\n",
    "def run_scraper(keyword=None, url=None, sort_by_date=True, download_pdfs=True):\n",
    "    \"\"\"Main scraping function, accepts keyword or URL and sorting preferences.\"\"\"\n",
    "    if not keyword and not url:\n",
    "        print(\"Please provide a keyword or URL\")\n",
    "        return\n",
    "\n",
    "    path_output = create_path(\"putusan\")\n",
    "    path_pdf = create_path(\"pdf-putusan\")\n",
    "    today_str = date.today().strftime(\"%Y-%m-%d\")\n",
    "    search_link = url if url else f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
    "\n",
    "    soup = fetch_html_with_retries(search_link)\n",
    "    last_page_number = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1][\"data-ci-pagination-page\"])\n",
    "\n",
    "    base_url = url or keyword\n",
    "    print(\n",
    "        f\"Scraping with {'url' if url else 'keyword'}: {base_url} - {20 * last_page_number} data - {last_page_number} page\")\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for page_number in range(1, last_page_number + 1):\n",
    "            futures.append(executor.submit(run_process, base_url, page_number, sort_by_date))\n",
    "    wait(futures)"
   ],
   "id": "8446088bde58ea36",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code executes the `run_scraper` function, which initiates a web scraping process targeting a specified URL related to Kelas 1A/Kelas 1A Khusus PN Makassar (Perdata Wanprestasi).",
   "id": "5ff03e1045f9904c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:57:38.336205Z",
     "start_time": "2024-12-10T03:12:38.762615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download Kelas 1A/Kelas 1A Khusus PN Makassar (Perdata Wanprestasi)\n",
    "run_scraper(url=\"https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=098111PN340+++++++++++++++++++++&t_put=2024&t_reg=&t_upl=&t_pr=\")"
   ],
   "id": "c0233fea348cc7ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping with url: https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=098111PN340+++++++++++++++++++++&t_put=2024&t_reg=&t_upl=&t_pr= - 620 data - 31 page\n",
      "Error occurred: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Download Kelas 1A/Kelas 1A Khusus PN Makassar (Perdata Wanprestasi)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mrun_scraper\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=098111PN340+++++++++++++++++++++&t_put=2024&t_reg=&t_upl=&t_pr=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 39\u001B[0m, in \u001B[0;36mrun_scraper\u001B[1;34m(keyword, url, sort_by_date, download_pdfs)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ThreadPoolExecutor(max_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m page_number \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, last_page_number \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 39\u001B[0m         futures\u001B[38;5;241m.\u001B[39mappend(executor\u001B[38;5;241m.\u001B[39msubmit(run_process, base_url, page_number, sort_by_date))\n\u001B[0;32m     40\u001B[0m wait(futures)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:637\u001B[0m, in \u001B[0;36mExecutor.__exit__\u001B[1;34m(self, exc_type, exc_val, exc_tb)\u001B[0m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, exc_type, exc_val, exc_tb):\n\u001B[1;32m--> 637\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshutdown\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwait\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\thread.py:235\u001B[0m, in \u001B[0;36mThreadPoolExecutor.shutdown\u001B[1;34m(self, wait, cancel_futures)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wait:\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads:\n\u001B[1;32m--> 235\u001B[0m         \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:1053\u001B[0m, in \u001B[0;36mThread.join\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1053\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1054\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1055\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[0;32m   1056\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[0;32m   1057\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:1073\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m   1070\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1072\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1073\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1074\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m   1075\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:57:38.502812700Z",
     "start_time": "2024-12-09T08:21:44.410417Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "47c5ef4a8540ccb8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
